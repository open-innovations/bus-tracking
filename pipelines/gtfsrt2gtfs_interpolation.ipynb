{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtfs_realtime_utils import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "ROOT = Path(\"../\")\n",
    "ROOT.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 20240923\n",
    "date_with_dashes = make_date_with_dashes(date)\n",
    "region = 'yorkshire'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the pre-processed real time data for the set date and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data = pd.read_csv(ROOT / f\"data/gtfs-rt/csv/{date}.csv\", low_memory=False)\n",
    "try:\n",
    "    next_day_data = pd.read_csv(ROOT / f\"data/gtfs-rt/csv/{date+1}.csv\", low_memory=False)\n",
    "    rt_data = pd.concat([day_data, next_day_data])\n",
    "except:\n",
    "    print('No next day of data to combine.')\n",
    "    rt_data = day_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a human-readable time column to sense check things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data['human_datetime'] = pd.to_datetime(rt_data['timestamp'], unit='s')\n",
    "rt_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fill in missing trip_ids using the following logic:\n",
    "- start at row 0. we need something to track which row we're on. lets say i.\n",
    "- if it has a trip id, go to the next row. keep a note of this row number, lets say k.\n",
    "- if the next row doesn't have a trip_id, go to the next row.\n",
    "- if the next row does have a trip id, compare that to the previou trip_id.\n",
    "- if they're the same, fill in all the previous ones with that trip_id.\n",
    "- if they're not, don't do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take the list of trip_ids, apply the function to fill the gaps, then re-apply the list of filled in trip_ids as the 'trip_id' column in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_trip_ids = rt_data.trip_id.to_list()\n",
    "filled_gaps_trip_ids = fill_trip_ids(list_of_trip_ids)\n",
    "print(fraction_with_trip_id(list_of_trip_ids))\n",
    "print(fraction_with_trip_id(filled_gaps_trip_ids))\n",
    "rt_data['trip_id'] = filled_gaps_trip_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above for start dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_start_dates = rt_data.start_date.to_list()\n",
    "filled_gaps_start_dates = fill_trip_ids(list_of_start_dates)\n",
    "rt_data['start_date'] = filled_gaps_start_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure we are only considering services that started on the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data = rt_data[rt_data.start_date == date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would keep all the data that has location info, but some of these still have no trip_id. There is no clear way to know which trip they are for. So, we will remove them from the data. These are always at the start/end of routes and so won't signifcantly affect our real time journey matching. In any case, we will fill in missing stops in the sequence using the timetable. So in the end, the lost data will not be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_trip_ids(df):\n",
    "    result = df[(df.stop_sequence != 0) & (df.trip_id != '')]\n",
    "    return result\n",
    "clean_rt_data = remove_missing_trip_ids(rt_data)\n",
    "print('Percentage of location data usable:', round(len(clean_rt_data)*100/len(rt_data), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to ensure there is only 1 timestamp per trip, per stop. \n",
    "\n",
    "Priority order is:\n",
    "1) `stopped at` - a bus is currently stopped at the stop\n",
    "2) `incoming at` - a bus is about to stop at the stop.\n",
    "3) `in transit to` - a bus is on its way to the stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is ordered by timestamp ascending downwards, so \"last\" is most recent and first is earliest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a status of 0 means the bus is incoming at the stop. Remove duplictaes, \n",
    "# taking the last (closest to stop) only.\n",
    "stopped_at = clean_rt_data.loc[clean_rt_data.status == 0].copy()\n",
    "stopped_at.drop_duplicates(subset=['trip_id', 'stop_sequence'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a status of 1 means the bus is stopped at the stop. Remove duplictaes, \n",
    "# taking the first (earliest time the bus was stopped at the stop) only.\n",
    "incoming_at = clean_rt_data.loc[clean_rt_data.status == 1].copy()\n",
    "incoming_at.drop_duplicates(subset=['trip_id', 'stop_sequence'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a status of 2 means the bus is in transit to the stop. Remove duplictaes, taking the last \n",
    "# (most recent time it was in transit to a stop) (closest to stop) only.\n",
    "in_transit = clean_rt_data.loc[clean_rt_data.status == 2].copy()\n",
    "in_transit.drop_duplicates(subset=['trip_id', 'stop_sequence'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-combine this data and re-sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([stopped_at, incoming_at, in_transit])\n",
    "combined.sort_values(by=['trip_id', 'stop_sequence', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pick one timestamp per trip based on the priority given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom sort key\n",
    "def prioritise_stop_status(data):\n",
    "    priority_mapping = {\"status\": {1: 0, 0: 1, 2: 2}}\n",
    "    data[\"priority\"] = data[\"status\"].map(priority_mapping[\"status\"])\n",
    "\n",
    "    # Group by and select the row with the highest priority\n",
    "    result = data.sort_values(by=[\"trip_id\", \"stop_sequence\", \"priority\"]) \\\n",
    "            .groupby([\"trip_id\", \"stop_sequence\"]) \\\n",
    "            .first().reset_index()\n",
    "\n",
    "    # Drop the helper column\n",
    "    result = result.drop(columns=[\"priority\"])\n",
    "    result.sort_values(by=['trip_id', 'stop_sequence', 'timestamp'], inplace=True)\n",
    "    return result\n",
    "\n",
    "combined_prioritised = prioritise_stop_status(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load the original timetable. We'll use the `stop_times.txt` file to compare with our live location data. The other files we will use a bit further down to create the \"real\" GTFS timetable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the timetable files are\n",
    "EXTRACT_DIR = ROOT / f\"18SepGB_GTFS_Timetables_Downloaded/{region}\"\n",
    "\n",
    "# Load the various timetable files into pandas dataframes.\n",
    "agencies, routes, trips, stops, stop_times, calendar, \\\n",
    "    calendar_dates, feed_info, shapes = load_full_gtfs(EXTRACT_DIR, include=['feed_info.txt', 'shapes.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This join shows us where we have gaps in the timetable and lets us work out which ones we can interpolate.\n",
    "complete_stop_time_data = stop_times.merge(combined_prioritised, on=['trip_id', 'stop_sequence'], how='left')\n",
    "simplified_stop_time_data = complete_stop_time_data.loc[:, \\\n",
    "    ['trip_id', 'arrival_time', 'departure_time', 'stop_sequence', 'timestamp', 'human_datetime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by 'trip_id' and keep rows that have at least 1 value in the \"timestamp\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'trip_id' and filter groups with at least one non-null 'timestamp'\n",
    "complete_stop_time_data = complete_stop_time_data.groupby('trip_id').filter(lambda group: group['timestamp'].notna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert times to UNIX timestamps\n",
    "complete_stop_time_data['unix_arrival_time'] = complete_stop_time_data['arrival_time'].apply(\\\n",
    "    lambda x: gtfs_time_to_unix_timestamp(x, date_with_dashes))\n",
    "\n",
    "complete_stop_time_data['unix_departure_time'] = complete_stop_time_data['departure_time'].apply(\\\n",
    "    lambda x: gtfs_time_to_unix_timestamp(x, date_with_dashes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_times(row):\n",
    "        r_n = row['timestamp']\n",
    "        t_n = row['unix_arrival_time']\n",
    "        # @TODO add interpolation for departure time as well. Can speed up by checking if the're\n",
    "        #  equivalent first and then doing calculation if needed.\n",
    "        r_pre = row['r_pre']\n",
    "        r_next = row['r_next']\n",
    "        t_pre = row['t_pre']\n",
    "        t_next = row['t_next']\n",
    "        # middle of values\n",
    "        # if current timestamp doesnt exist, but next and previous values do.\n",
    "        if pd.isna(r_n):\n",
    "            if not pd.isna(t_n) and not pd.isna(r_pre) and not pd.isna(r_next) and not pd.isna(t_pre) and not pd.isna(t_next):\n",
    "                try:\n",
    "                    interpolated_time = round(r_pre + (((t_n - t_pre)/(t_next - t_pre)) * (r_next - r_pre)), 0)\n",
    "                except:\n",
    "                    #  If the above fails e.g. in the case of VJb3e8a46add17e4a54287ae84eaf808488812ddaa Yorkshire, due to timetabling error.\n",
    "                     interpolated_time = np.nan\n",
    "                return interpolated_time\n",
    "            \n",
    "            ### REMOVING THIS FOR NOW - we don't want to extrapolate values.\n",
    "\n",
    "            # # deal with first missing values. (due to no start real time)\n",
    "            # # We use the next time to work backwards\n",
    "            # if pd.isna(r_pre) and not pd.isna(r_next) and not pd.isna(t_pre) and not pd.isna(t_next):\n",
    "            #      interpolated_time = round(t_pre + ((r_next - t_pre) / (t_next - t_pre)), 0)\n",
    "            #      return interpolated_time\n",
    "            \n",
    "            # # deal with end stops (due to no end real time appearing)\n",
    "            # # We use the latest available time to fill forwards.\n",
    "            # if pd.isna(r_next) and not pd.isna(t_next) and not pd.isna(t_pre):\n",
    "            #     interpolated_time = round(t_n + (r_pre - t_pre), 0)\n",
    "            #     return interpolated_time\n",
    "            \n",
    "        else:\n",
    "            return r_n\n",
    "        \n",
    "def fill_real_time(group):\n",
    "    # Add a segment ID that increments after each NA in `timestamp` \n",
    "    group['segment'] = group['timestamp'].notna().cumsum()\n",
    "    group['t_pre'] = group.groupby('segment')['unix_arrival_time'].transform('first')\n",
    "\n",
    "    # Group by 'segment' and get the first 'arrival_time' for each group\n",
    "    first_arrival = group.groupby('segment')['unix_arrival_time'].first().shift(-1)\n",
    "    # Map the shifted values back to the DataFrame\n",
    "    group['t_next'] = group['segment'].map(first_arrival)\n",
    "    # Fill NaN in the last segment with the last 'arrival_time'\n",
    "    group['t_next'] = group['t_next'].fillna(group['unix_arrival_time'].iloc[-1])\n",
    "    # Add columns for previous and next values\n",
    "    group['r_pre'] = group['timestamp'].shift(1).ffill() \n",
    "    # Shift down for the previous value\n",
    "    group['r_next'] = group['timestamp'].shift(-1).bfill()\n",
    "\n",
    "    for i, row in group.iterrows():\n",
    "        group.at[i, 'interpolated_time'] = interpolate_missing_times(row)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the fill_real_time function group-wise\n",
    "mixed_real_interpolated_data = complete_stop_time_data.groupby('trip_id', group_keys=False).apply(fill_real_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_real_interpolated_data['final_arrival_time'] = mixed_real_interpolated_data['timestamp'].combine_first(\\\n",
    "    mixed_real_interpolated_data['interpolated_time'])\n",
    "\n",
    "mixed_real_interpolated_data['final_departure_time'] = mixed_real_interpolated_data['timestamp'].combine_first(\\\n",
    "    mixed_real_interpolated_data['interpolated_time'])\n",
    "\n",
    "# Add a source column to indicate which column the value came from\n",
    "mixed_real_interpolated_data['interpolated'] = mixed_real_interpolated_data.apply(\n",
    "    lambda row: '0' if pd.notna(row['timestamp']) else ('1' if pd.notna(row['interpolated_time']) else None), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any rows that don't have either a real or interpolated time (NA values).\n",
    "mixed_real_interpolated_data = mixed_real_interpolated_data[\\\n",
    "    (mixed_real_interpolated_data.final_arrival_time.notna()) & \\\n",
    "    (mixed_real_interpolated_data.final_departure_time.notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "tz = int(tz_offset(str(date), geo='Europe/London'))\n",
    "mixed_real_interpolated_data['final_gtfs_arrival_time'] = unix_to_gtfs_time( mixed_real_interpolated_data['final_arrival_time'], date_with_dashes, tz=tz)\n",
    "mixed_real_interpolated_data['final_gtfs_departure_time'] = unix_to_gtfs_time(mixed_real_interpolated_data['final_departure_time'], date_with_dashes, tz=tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'arrival_time' and 'departure_time' as the final corrected time column.\n",
    "mixed_real_interpolated_data['arrival_time'] = mixed_real_interpolated_data['final_gtfs_arrival_time']\n",
    "mixed_real_interpolated_data['departure_time'] = mixed_real_interpolated_data['final_gtfs_departure_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_timetable = mixed_real_interpolated_data.loc[:, ['trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence',\n",
    "                                                          'stop_headsign', 'pickup_type', 'drop_off_type', 'shape_dist_traveled',\n",
    "                                                          'timepoint', 'route_id', 'interpolated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_DIR = ROOT / f\"data/gtfs/real-interpolated/{region}/{date}\"\n",
    "os.makedirs(os.path.abspath(REAL_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealRouteIDs = realtime_timetable['route_id']\n",
    "RealRoutes = routes[routes['route_id'].isin(RealRouteIDs)]\n",
    "RealRoutes.to_csv(REAL_DIR / \"routes.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealAgencyIDs = routes['agency_id']\n",
    "RealAgencies = agencies[agencies['agency_id'].isin(RealAgencyIDs)]\n",
    "RealAgencies.to_csv(REAL_DIR / \"agency.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealTripIDs = realtime_timetable['trip_id']\n",
    "RealTrips = trips[trips['trip_id'].isin(RealTripIDs)]\n",
    "RealTrips.to_csv(REAL_DIR / \"trips.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealShapeIDs = RealTrips['shape_id'].unique()\n",
    "RealShapes = shapes[shapes.shape_id.isin(RealShapeIDs)]\n",
    "RealShapes.to_csv(REAL_DIR / \"shapes.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealServiceIDs = RealTrips['service_id'].unique()\n",
    "RealCalendar = calendar[calendar['service_id'].isin(RealServiceIDs)]\n",
    "RealCalendar.to_csv(REAL_DIR / \"calendar.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealCalendarDates = calendar_dates[calendar_dates['service_id'].isin(RealServiceIDs)]\n",
    "RealCalendarDates.to_csv(REAL_DIR / \"calendar_dates.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only columns we need to write stop_times.txt\n",
    "RealStopTimes = realtime_timetable.loc[:, ['trip_id','arrival_time','departure_time','stop_id','stop_sequence','stop_headsign','pickup_type','drop_off_type','shape_dist_traveled','timepoint','interpolated']]\n",
    "RealStopTimes.to_csv(REAL_DIR / \"stop_times.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealStopIDs = realtime_timetable['stop_id'].to_list()\n",
    "RealStops = stops[stops.stop_id.isin(RealStopIDs)].copy()\n",
    "\n",
    "# Location type must be able to be parsed as an integer.\n",
    "RealStops['location_type'] = RealStops['location_type'].astype('Int64')\n",
    "RealStops.drop(columns='parent_station', inplace=True)\n",
    "RealStops.to_csv(REAL_DIR / \"stops.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing feed_info.txt\n",
    "feed_info.to_csv(REAL_DIR / \"feed_info.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the GTFS timetables into a single GTFS.zip file.\n",
    "zip_directory(REAL_DIR, ROOT / 'data/real-interpolated', f'{region}_{date}.gtfs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus-tracking-JZQiYmLK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
