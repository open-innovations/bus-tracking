{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info\n",
    "This script was our original method of updating timetables with live location data. It's based on calculating the distance to stops and checking the bearing of the bus in relation to stops. This works well when the location data doesn't contain information about which stop the bus is close to.\n",
    "\n",
    "We've since moved to a method that uses the `current_stop_status` and `current_stop_sequence` provided by the GTFS-RT format, and also utilises interpolation to fill in stops that we don't match to. This is detailed in `gtfsrt2gtfs_interpolation.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtfs_realtime_utils import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the root path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\"../\")\n",
    "ROOT.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a date and a region.\n",
    "date must be ISO8061 format.\n",
    "region must be the name of the region.\n",
    "Your timetable GTFS file must be follow the name convention `<date>_<region>.gtfs.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso8061_date = '20240923'\n",
    "region = 'north_west'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the timetables are unzipped so we can read them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMETABLE_FILE = ROOT / f\"data/gtfs/timetable/{region}_20240918.gtfs.zip\"\n",
    "\n",
    "# Define the directory where you want to extract files from the timetable\n",
    "EXTRACT_DIR = ROOT / f\"18SepGB_GTFS_Timetables_Downloaded/{region}\"\n",
    "\n",
    "unzip_file(os.path.abspath(TIMETABLE_FILE), EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "To be able to match the live buses to a trip/route etc, we need a way to lookup a given bus's `trip_id` and find its timetabled information. We'll do this using dictionaries because they have [O(1) lookup time thanks to hashmaps](https://dev.to/ajipelumi/how-dictionary-lookup-operations-are-o1-49pk).\n",
    "\n",
    "We load just the parts we need into pandas dataframes, then use the `to_dict()` method to create our dictionaries for various ID->ID combinations.\n",
    "\n",
    "We can get more detailed information about UK stops from the [National Public Transport Access Codes data](https://beta-naptan.dft.gov.uk/download). We need the bearing to help match buses to stops on the correct side of the road for their direction of travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ID parts of various files that we extracted from the GTFS timetable.\n",
    "agencies, routes, stops, stop_times, trips = load_gtfs_ids(EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stop names and bearings for UK stops\n",
    "stop_names_bearings = get_stop_names_and_bearings(ROOT)\n",
    "stop_names_bearings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dataframes to be used to make dicts\n",
    "agency2route = agencies.merge(routes, on='agency_id').set_index('agency_id')\n",
    "AgencyNOC2AgencyIDDict = agency2route['agency_noc'].drop_duplicates().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each trip_id has multiple associated stops. Aggregate these into a list per trip_id.\n",
    "trip2stoptimes = trips.merge(stop_times, on='trip_id')[['trip_id', 'stop_id']].groupby('trip_id').agg(list)\n",
    "Trip2StopIDDict = trip2stoptimes['stop_id'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some stop_id in stops.txt that are not in stop_times.txt. \n",
    "# We can't use these. Our merge removes these as we complete an \"inner\" join.\n",
    "stoptimes2stops = stop_times.merge(stops, on='stop_id').merge(stop_names_bearings, how='inner', on='stop_id')\n",
    "stoptimes2stops.drop_duplicates(subset='stop_id', keep='first', inplace=True) # We only need unique stop_ids\n",
    "stoptimes2stops.set_index('stop_id', inplace=True)\n",
    "StopID2StopLocDict = stoptimes2stops[['stop_lat', 'stop_lon', 'Bearing']].to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the paths of the GTFS-RT files.\n",
    "\n",
    "For each of the extracted GTFS-RT files we get their full paths and save it in `gtfs_rt_file_paths`. This is to save some compute time downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALTIME_DATADIR = ROOT / f\"data/gtfs-rt\"\n",
    "date_with_dashes = f\"{iso8061_date[0:4]}-{iso8061_date[4:6]}-{iso8061_date[6:8]}\"\n",
    "# Create an empty list to store file paths\n",
    "gtfs_rt_file_paths = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(REALTIME_DATADIR):\n",
    "    for file in files:\n",
    "        # Get the full path of the file and append it to the list\n",
    "        if file[0:10] == date_with_dashes:\n",
    "            full_path = os.path.abspath(os.path.join(root, file))\n",
    "            gtfs_rt_file_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the feed entities \n",
    "We load each feed entity into a list (array in most other languages) to loop through later. We could write this in one big loop, but we've split it up to save compute time and make the code easier to follow/debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = gtfs_realtime_pb2.FeedMessage()\n",
    "entities = entities_to_list(feed, gtfs_rt_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of the following algorithm can be found in the readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BusDetailsBag = []\n",
    "count = 0\n",
    "t0 = time.time()\n",
    "# For the moment assuming that `vehicle` is passed. there are other mutually exclusive \n",
    "# options: 'trip_update', 'alert', and 'shape'. See the DOCS https://gtfs.org/documentation/realtime/reference/#message-feedentity\n",
    "for entity in entities:\n",
    "    BD = BusDetail()\n",
    "    # These are the two main parts of entity\n",
    "    vehicle = entity.vehicle\n",
    "    # # These are sub parts\n",
    "    trip = vehicle.trip\n",
    "    pos = vehicle.position\n",
    "    # These are individual values\n",
    "    BD.feed_uid = entity.id\n",
    "    BD.trip_id = trip.trip_id\n",
    "    BD.route_id = trip.route_id\n",
    "    BD.lat = round(pos.latitude, 6)\n",
    "    BD.lon = round(pos.longitude, 6)\n",
    "    BD.bearing = pos.bearing\n",
    "    BD.ts = vehicle.timestamp\n",
    "    BD.v_id = vehicle.vehicle.id\n",
    "    BD.occupancy_status = vehicle.occupancy_status\n",
    "    BD.current_stop_sequence = vehicle.current_stop_sequence\n",
    "    BD.current_status = vehicle.current_status\n",
    "    BD.start_time = trip.start_time\n",
    "    BD.start_date = trip.start_date\n",
    "\n",
    "    if BD.trip_id:\n",
    "        stops_on_route = Trip2StopIDDict.get(BD.trip_id)\n",
    "        # If stops isn't none - i.e. if this trip ID is part of the timetable\n",
    "        if stops_on_route:\n",
    "            # Get the stop_id, lat, and lon for each stop on the route.\n",
    "            # Scaling the longitudes because they get closer together nearer the poles. Could replace BD.lat with cos(~53) for UK average\n",
    "            actual_stop_locations_on_route = [\n",
    "                (stopid, StopID2StopLocDict[stopid]['stop_lat'], \n",
    "                 StopID2StopLocDict[stopid]['stop_lon'], \n",
    "                 abs(StopID2StopLocDict[stopid]['Bearing'] - BD.bearing), \n",
    "                 abs(StopID2StopLocDict[stopid]['stop_lat'] - BD.lat), \n",
    "                 abs((StopID2StopLocDict[stopid]['stop_lon'] - BD.lon)/np.cos(BD.lat))\n",
    "                ) for stopid in stops_on_route\n",
    "            ]\n",
    "            \n",
    "            box_size = 0.003 # 0.01 is ~1.1km \n",
    "            \n",
    "            # Filter out all stops that are pointing the \"wrong direction\". Absolute difference between stop bearing and bus bearing should be < 90 degrees. This gives a semi-circle's worth of error margin.\n",
    "            stops_in_bounds = [item for item in actual_stop_locations_on_route if (item[4] < box_size) and (item[5] < box_size) and (item[3] < 90) and (item[3] != 'nan')]\n",
    "            \n",
    "            if len(stops_in_bounds) > 0:\n",
    "                candidate_stops_and_distances = [item + (haversine(BD.lat, BD.lon, item[1], item[2]),) for item in stops_in_bounds]\n",
    "    \n",
    "                n_possible_stops = len(candidate_stops_and_distances)\n",
    "                if n_possible_stops == 1:\n",
    "                    # Found the nearest stop already. Get the distance and stop_id.\n",
    "                    closest_stop_id = candidate_stops_and_distances[0][0]\n",
    "                    closest_stop_distance = candidate_stops_and_distances[0][6]\n",
    "                else:\n",
    "                    # Need to get min distance from n stops.\n",
    "                    index_of_smallest_distance = min(range(len(candidate_stops_and_distances)), key=lambda i: candidate_stops_and_distances[i][6])\n",
    "                    closest_stop_id = candidate_stops_and_distances[index_of_smallest_distance][0]\n",
    "                    closest_stop_distance = candidate_stops_and_distances[index_of_smallest_distance][6]\n",
    "                \n",
    "                # Add the details to the current bus.\n",
    "                BD.NearestStopOnRoute = closest_stop_id\n",
    "                BD.NearestStopDistance = closest_stop_distance * 1e3 #convert to m\n",
    "\n",
    "                # Ensure we found a nearest stop and that the bus is \"sufficiently\" close.\n",
    "                if BD.NearestStopOnRoute != None and BD.NearestStopDistance < 200:\n",
    "                    BusDetailsBag.append(BD)\n",
    "\n",
    "    # Creating some info to see progress\n",
    "    if count!= 0 and count % 500000 == 0:\n",
    "        t1 = time.time()\n",
    "        print('Time elapsed:', round(t1-t0, 3), 's')\n",
    "        print(f\"{count} of {len(entities)} entities parsed.\")\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'trip_id': bus.trip_id, \"route_id\": bus.route_id, 'timestamp': bus.ts, 'nearest_stop_id': bus.NearestStopOnRoute, 'distance': bus.NearestStopDistance} for bus in BusDetailsBag]\n",
    "\n",
    "# Add the data to a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Work out the human readable time\n",
    "df['human_time'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n",
    "# Timezone is currently BST =  UTC + 1, so need to add 1 hour.\n",
    "df['uk_bst_time_only'] = (df['human_time'] + pd.Timedelta(hours=1)).dt.strftime('%H:%M:%S')\n",
    "\n",
    "# Weird rows where they aren't the right date. Binning them\n",
    "df = df.loc[df.human_time.dt.date == pd.to_datetime(f'{iso8061_date[0:4]}-{iso8061_date[4:6]}-{iso8061_date[6:8]}').date()]\n",
    "\n",
    "# only remove duplictaes that have same stop_id, route_id and nearest_stop_id\n",
    "df.drop_duplicates(subset=['trip_id', 'route_id', 'nearest_stop_id'], keep='first', inplace=True)\n",
    "\n",
    "FilteredOrderedBusLocations = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agencies, routes, trips, stops, stop_times, calendar, calendar_dates, feed_info, shapes = load_full_gtfs(EXTRACT_DIR, include=['feed_info.txt', 'shapes.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_DIR = ROOT / f\"data/gtfs/real/{region}/{iso8061_date}\"\n",
    "os.makedirs(os.path.abspath(REAL_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealRouteIDs = FilteredOrderedBusLocations['route_id'].astype(int)\n",
    "RealRoutes = routes[routes['route_id'].isin(RealRouteIDs)]\n",
    "RealRoutes.to_csv(REAL_DIR / \"routes.txt\", index=False)\n",
    "RealRoutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealAgencyIDs = routes['agency_id']\n",
    "RealAgencies = agencies[agencies['agency_id'].isin(RealAgencyIDs)]\n",
    "RealAgencies.to_csv(REAL_DIR / \"agency.txt\", index=False)\n",
    "RealAgencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealTripIDs = FilteredOrderedBusLocations['trip_id']\n",
    "RealTrips = trips[trips['trip_id'].isin(RealTripIDs)]\n",
    "RealTrips.to_csv(REAL_DIR / \"trips.txt\", index=False)\n",
    "RealTrips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealShapeIDs = RealTrips['shape_id'].unique()\n",
    "RealShapes = shapes[shapes.shape_id.isin(RealShapeIDs)]\n",
    "RealShapes.to_csv(REAL_DIR / \"shapes.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealServiceIDs = RealTrips['service_id'].unique()\n",
    "RealCalendar = calendar[calendar['service_id'].isin(RealServiceIDs)]\n",
    "RealCalendarDates = calendar_dates[calendar_dates['service_id'].isin(RealServiceIDs)]\n",
    "RealCalendar.to_csv(REAL_DIR / \"calendar.txt\", index=False)\n",
    "RealCalendarDates.to_csv(REAL_DIR / \"calendar_dates.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealTripIDs_list = RealTripIDs.to_list()\n",
    "FilteredStopTimes = stop_times[stop_times.trip_id.isin(RealTripIDs_list)]\n",
    "FilteredOrderedBusLocations.rename(columns={'nearest_stop_id': 'stop_id'}, inplace=True)\n",
    "RealTrips2RealStopsDict = FilteredStopTimes.groupby('trip_id')['stop_id'].agg(list).to_dict()\n",
    "\n",
    "RealStopTimes = FilteredStopTimes.merge(FilteredOrderedBusLocations, on=['trip_id', 'stop_id'], how='inner')\n",
    "RealStopTimes['arrival_time'] = RealStopTimes['uk_bst_time_only']\n",
    "RealStopTimes['departure_time'] = RealStopTimes['uk_bst_time_only']\n",
    "\n",
    "# A trip must visit more than one stop in stop_times.txt to be usable by passengers for boarding and alighting.\n",
    "stop_counts = RealStopTimes.groupby('trip_id')['stop_id'].count()\n",
    "trip_ids_with_one_stop = stop_counts[stop_counts == 1].index.to_list()\n",
    "RealStopTimes = RealStopTimes[~RealStopTimes.trip_id.isin(trip_ids_with_one_stop)] # Exclude trips with only 1 stop. (The \"~\" is negation)\n",
    "\n",
    "# When sorted by stop_times.stop_sequence, two consecutive entries in stop_times.txt \n",
    "# should have increasing distance, based on the field shape_dist_traveled. \n",
    "# If the values are equal, this is considered as an error.\n",
    "RealStopTimes.sort_values(by=['trip_id', 'stop_sequence'], ascending=True, inplace=True)\n",
    "\n",
    "# If pick up type == 1, no pickup is available (Guessing this means you can't get on the bus here?)\n",
    "RealStopTimes = RealStopTimes[RealStopTimes.pickup_type != 1]\n",
    "\n",
    "# Remove duplicate rows based on the specified columns but keep the first occurrence\n",
    "RealStopTimes = RealStopTimes.drop_duplicates(subset=['trip_id', 'stop_id', 'arrival_time', 'departure_time', 'shape_dist_traveled'], keep='first')\n",
    "\n",
    "# Filtering only columns we need to write stop_times.txt\n",
    "RealStopTimes = RealStopTimes[['trip_id','arrival_time','departure_time','stop_id','stop_sequence','stop_headsign','pickup_type','drop_off_type','shape_dist_traveled','timepoint']]\n",
    "\n",
    "RealStopTimes.to_csv(REAL_DIR / \"stop_times.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RealStopIDs = FilteredOrderedBusLocations['stop_id'].to_list()\n",
    "RealStops = stops[stops.stop_id.isin(RealStopIDs)].copy()\n",
    "\n",
    "# Location type must be able to be parsed as an integer.\n",
    "RealStops['location_type'] = RealStops['location_type'].astype('Int64')\n",
    "RealStops.drop(columns='parent_station', inplace=True)\n",
    "RealStops.to_csv(REAL_DIR / \"stops.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_info.to_csv(REAL_DIR / \"feed_info.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def zip_directory(folder_path, output_dir_path, output_filename):\n",
    "    # Ensure the output filename does not have a .zip extension\n",
    "    if not output_filename.endswith('.zip'):\n",
    "        output_filename += '.zip'\n",
    "    # Join the output directory and filename\n",
    "    output_zip_path = output_dir_path / output_filename\n",
    "    \n",
    "    # Create the zip archive in the specified directory\n",
    "    shutil.make_archive(output_zip_path.with_suffix(''), 'zip', folder_path)\n",
    "    print(f\"Directory '{folder_path}' successfully zipped as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_directory(REAL_DIR, ROOT / 'data/real', f'{region}_{iso8061_date}.gtfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_entity_metadata(entities, ROOT / f'data/gtfs-rt/metadata/{region}_{iso8061_date}.csv')\n",
    "metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus-tracking-JZQiYmLK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
